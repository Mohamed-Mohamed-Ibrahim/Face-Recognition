{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"authorship_tag":"ABX9TyMoOKSV+doW81YKLNOSgr5K","include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Mohamed-Mohamed-Ibrahim/Face-Recognition/blob/main/GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"!pip install torch","metadata":{"id":"77miQoqHA3Ou","outputId":"5e6a29a6-34b0-4cb6-e9b5-6b353699d5dc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nX = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\ngm = GaussianMixture(n_components=2, random_state=0).fit(X)\ngm.means_\ngm.predict([[0, 0], [12, 3]])","metadata":{"id":"KuR0EHcNA4Ty","outputId":"f673501b-99cc-4d68-c855-42102669d746"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\nX = np.array([np.arange(28)])\ny = np.array([])\n\n# Define the transformation\ntransform = transforms.ToTensor()\n\n# Load the dataset\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n\n# Create a subplot with 4x4 grid\nfig, axs = plt.subplots(4, 4, figsize=(8, 8))\n\n# Loop through each subplot and plot an image\nfor i in range(4):\n    for j in range(4):\n        image, label = train_dataset[i * 4 + j]  # Get image and label\n        image_numpy = image.numpy().squeeze()    # Convert image tensor to numpy array\n        # print(len(image_numpy))\n        X = np.append(X, image_numpy, axis=0)\n        y = np.append(y, label)\n        axs[i, j].imshow(image_numpy, cmap='gray')  # Plot the image\n        axs[i, j].axis('off')  # Turn off axis\n        axs[i, j].set_title(f\"Label: {label}\")  # Set title with label\nX = np.delete(X, 0, axis=0)\n\nplt.tight_layout()  # Adjust layout\nplt.show()  # Show plot\n","metadata":{"id":"7VpbetarBA6l","outputId":"390ee890-ea91-405c-a8a3-0b26e4309a81"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X: {X}, size: {X.shape}\")\nprint(f\"y: {y}, size: {len(y)}\")","metadata":{"id":"pCibb8kWFExm","outputId":"38194b16-6e8a-4f8d-d2a8-5ec7a1ec1a27"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"40 + np.zeros(4)","metadata":{"id":"YsNj7IQdHp1q","outputId":"21fe4b05-664a-4bf6-b83f-75277b6767b6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ipcJlAPPJmqo"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"Ypz1B5glJnu2"}},{"cell_type":"code","source":"","metadata":{"id":"FykfwLXhJmtF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\n\n\nclass GMM(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-06, max_iter=100, verbose=False):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.verbose = verbose\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n\n    def fit(self, X):\n        best_m = None\n        best_c = None\n        best_w = None\n        best_log_likelihood = -np.inf\n\n        for _ in range(10):\n            self.__fit(X)\n            ll = self.log_likelihood(X)\n            if ll > best_log_likelihood:\n                best_log_likelihood = ll\n                best_m = self.means_\n                best_c = self.covariances_\n                best_w = self.weights_\n            self.means_ = None\n            self.covariances_ = None\n            self.weights_ = None\n        self.means_ = best_m\n        self.covariances_ = best_c\n        self.weights_ = best_w\n\n\n    def __fit(self, X):\n\n        n_samples, n_features = X.shape\n        self.means_ = np.random.rand(self.n_components, n_features)*20 - 10\n        self.covariances_ = np.zeros((self.n_components, n_features, n_features))\n        for i in range(self.n_components):\n            self.covariances_[i] = np.eye(n_features)\n        self.weights_ = (1/self.n_components) + np.zeros(self.n_components)\n\n        r = np.zeros((self.n_components, n_samples))\n\n        for _ in range(self.max_iter):\n\n            if self.verbose and self.max_iter % 50 == 0:\n                print(f\"Iteration: {_} -> Means {self.means_} -> Var {self.covariances_} -> Weights {self.weights_}\")\n\n            # E-step\n            # Assign r using posterior probability\n            for i in range(self.n_components):\n                r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, self.means_[i], self.covariances_[i])\n\n            # Normalization\n            for i in range(n_samples):\n                r[:, i] = r[:, i] / np.sum(r[:, i])\n\n            old_w = self.weights_.copy()\n            old_m = self.means_.copy()\n            old_c = self.covariances_.copy()\n\n            # M-step\n            for i in range(self.n_components):\n\n                self.weights_[i] = np.sum(r[i, :]) / n_samples\n\n                for j in range(n_features):\n                    self.means_[i][j] = np.sum(r[i, :] * X[:, j]) / np.sum(r[i, :])\n                # self.means_[i] = (r[i, :, np.newaxis] * X).sum(axis=0) / np.sum(r[i, :])\n                    for k in range(n_features):\n                        self.covariances_[i][j, k] = np.sum(r[i, :] * (X[:, j] - self.means_[i][j]) * (X[:, k] - self.means_[i][k])) / np.sum(r[i, :])\n\n                # diff = X - self.means_[i]\n                # self.covariances_[i] = (r[i, :, np.newaxis, np.newaxis] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / np.sum(r[i, :])\n\n                # Regularize covariance\n                self.covariances_[i] += self.reg_covar * np.eye(n_features)\n\n            if np.all(np.abs(old_w - self.weights_) < self.tol) and np.all(np.abs(old_m - self.means_) < self.tol) and np.all(np.abs(old_c - self.covariances_) < self.tol):\n                break\n\n\n    def predict(self, X):\n        n_samples = len(X)\n\n        r = np.zeros((self.n_components, n_samples))\n\n        for i in range(self.n_components):\n            r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, self.means_[i], self.covariances_[i])\n\n        return np.argmax(r, axis=0)\n\n    def log_likelihood(self, X):\n        '''\n        Compute the log-likelihood of X under current parameters\n        input:\n            - X: Data (batch_size, dim)\n        output:\n            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))\n        '''\n        ll = []\n        for d in X:\n            tot = 0\n            for k in range(self.n_components):\n                tot += self.weights_[k] * multivariate_normal.pdf(X, mean=self.means_[k], cov=self.covariances_[k])\n            ll.append(np.log(tot))\n        return np.sum(ll)","metadata":{"id":"q49lTRmbJofl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"09lIUczMhBGu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ErA9QB6khA5A"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.zeros((1, 2))","metadata":{"id":"l0huEmy9Xqka","outputId":"6469b4b0-44a2-45a0-ea96-281d6c4fb8ea"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nX = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\ngm = GaussianMixture(n_components=2, verbose=1).fit(X)\nprint(gm.means_)\nprint(gm.covariances_)\nprint(gm.weights_)\ngm.predict([[0, 0], [12, 3]])","metadata":{"id":"zZOQeSXyJoim","outputId":"0620105d-7683-44bf-bc18-2904131b10b0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\ngmm = GMM(n_components=2)\ngmm.fit(X)\nprint(gmm.means_)\nprint(gmm.covariances_)\nprint(gmm.weights_)\nprint(gmm.predict([[0, 0], [12, 3]]))\ngmm.log_likelihood(X)","metadata":{"id":"ztwxmVylJolX","outputId":"a08d1fb1-2329-4f6b-d49c-0e3032fbeb84"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resourses","metadata":{"id":"5-rlhdDCQDUh"}},{"cell_type":"code","source":"","metadata":{"id":"JDxFboeOagKa"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"https://www.geeksforgeeks.org/how-to-load-fashion-mnist-dataset-using-pytorch/\nhttps://youtu.be/wT2yLNUfyoM\nhttps://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\nhttps://github.com/scikit-learn/scikit-learn/blob/98ed9dc73/sklearn/mixture/_gaussian_mixture.py#L509","metadata":{"id":"hVGfGllPQDe3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nX = np.array([[1], [1], [1], [10], [10], [10]])\ngm = GaussianMixture(n_components=2, random_state=0).fit(X)\nprint(gm.means_)\nprint(gm.covariances_)\nprint(gm.weights_)\ngm.predict([[0], [12]])","metadata":{"id":"_GOfaHu4D687","outputId":"e99c0cdf-4554-4d43-db26-a166e74e4656"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.zeros(1, 3, 2)","metadata":{"id":"DzYoUbggD-BR","outputId":"f20baf5d-6408-480d-a172-d9095d1d7ad3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nX, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.70, random_state=0)\n\nplt.scatter(X[:, 0], X[:, 1], s=30, cmap='viridis')\nplt.title(\"Simple Blobs Dataset\")\nplt.show()\n","metadata":{"id":"C06vz8ZhHIET","outputId":"7df07414-09c7-4f9a-df57-e3a7da057279"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gmm = GMM(n_components=3, tol=1e-9, max_iter=100)\ngmm.fit(X)\nlabels = gmm.predict(X)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=15, cmap='viridis')\nplt.title(\"GMM Clustering Result\")\nplt.show()\n","metadata":{"id":"n-kif2WFaiaR","outputId":"8e347a0d-4335-4d9d-c89c-7dee8e24c77f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chatgpt Implementation","metadata":{"id":"r9e4Mh9KbKqW"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\n\n\nclass GMM(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-6, max_iter=100):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n\n    def fit(self, X):\n        n_samples, n_features = X.shape\n        self.means_ = np.random.rand(self.n_components, n_features) * 20 - 10\n        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n        self.weights_ = np.ones(self.n_components) / self.n_components\n\n        r = np.zeros((self.n_components, n_samples))\n\n        for iteration in range(self.max_iter):\n            # print(f\"Iteration {iteration}: Means {self.means_}\")\n\n            # E-step\n            for k in range(self.n_components):\n                r[k, :] = self.weights_[k] * multivariate_normal.pdf(X, mean=self.means_[k], cov=self.covariances_[k])\n\n            r /= np.sum(r, axis=0)  # Normalize responsibilities\n\n            # Save old parameters\n            old_means = self.means_.copy()\n            old_covariances = self.covariances_.copy()\n            old_weights = self.weights_.copy()\n\n            # M-step\n            Nk = np.sum(r, axis=1)  # Effective number of points assigned to each cluster\n\n            for k in range(self.n_components):\n                self.weights_[k] = Nk[k] / n_samples\n                self.means_[k] = np.sum(r[k, :, np.newaxis] * X, axis=0) / Nk[k]\n\n                diff = X - self.means_[k]\n                self.covariances_[k] = (r[k, :, np.newaxis, np.newaxis] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk[k]\n\n                # Regularize covariance\n                self.covariances_[k] += self.reg_covar * np.eye(n_features)\n\n            # Convergence check\n            if (np.all(np.abs(self.weights_ - old_weights) < self.tol) and\n                np.all(np.abs(self.means_ - old_means) < self.tol) and\n                np.all(np.abs(self.covariances_ - old_covariances) < self.tol)):\n                print(\"Converged.\")\n                break\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        r = np.zeros((self.n_components, n_samples))\n\n        for k in range(self.n_components):\n            r[k, :] = self.weights_[k] * multivariate_normal.pdf(X, mean=self.means_[k], cov=self.covariances_[k])\n\n        return np.argmax(r, axis=0)\n\n    def log_likelihood(self, X):\n        '''\n        Compute the log-likelihood of X under current parameters\n        input:\n            - X: Data (batch_size, dim)\n        output:\n            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))\n        '''\n        ll = []\n        for d in X:\n            tot = 0\n            for k in range(self.n_components):\n                tot += self.weights_[k] * multivariate_normal.pdf(X, mean=self.means_[k], cov=self.covariances_[k])\n            ll.append(np.log(tot))\n        return np.sum(ll)\n","metadata":{"id":"DK-REZIQan6g"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\n\n\nclass GMMs(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-06, max_iter=100, covariance_type='full'):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.covariance_type = covariance_type\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n\n    def fit(self, X):\n        n_samples, n_features = X.shape\n        self.means_ = np.random.rand(self.n_components, n_features) * 20 - 10  # Random initialization\n        self.covariances_ = np.zeros((self.n_components, n_features, n_features))\n        for i in range(self.n_components):\n            self.covariances_[i] = np.eye(n_features)\n        self.weights_ = np.full(self.n_components, 1 / self.n_components)  # Uniform weights\n\n        r = np.zeros((self.n_components, n_samples))  # Responsibility matrix\n\n        for _ in range(self.max_iter):\n            # E-step: Compute responsibilities\n            for i in range(self.n_components):\n                r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, self.means_[i], self.covariances_[i])\n\n            # Normalize responsibilities (posterior probabilities)\n            r /= np.sum(r, axis=0)\n\n            # Store old parameters for convergence check\n            old_w = self.weights_.copy()\n            old_m = self.means_.copy()\n            old_c = self.covariances_.copy()\n\n            # M-step: Update parameters\n            for i in range(self.n_components):\n                Nk = np.sum(r[i, :])  # Effective number of points assigned to component i\n\n                # Update weights\n                self.weights_[i] = Nk / n_samples\n\n                # Update means\n                self.means_[i] = np.sum(r[i, :, np.newaxis] * X, axis=0) / Nk\n\n                # Update covariances based on the covariance type\n                diff = X - self.means_[i]\n                if self.covariance_type == 'full':\n                    self.covariances_[i] = (r[i, :, np.newaxis, np.newaxis] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk\n                elif self.covariance_type == 'diagonal':\n                    self.covariances_[i] = np.diag(np.sum(r[i, :] * np.square(diff), axis=0) / Nk)\n                elif self.covariance_type == 'tied':\n                    if i == 0:\n                        self.covariances_[i] = np.zeros((n_features, n_features))\n                    self.covariances_[i] += (r[i, :, np.newaxis] * diff).T @ diff / Nk\n                else:\n                    raise ValueError(\"Unknown covariance_type. Choose from 'full', 'diagonal', 'tied'.\")\n\n                # Regularize covariance matrix to ensure it's positive definite\n                self.covariances_[i] += self.reg_covar * np.eye(n_features)\n\n            # Check for convergence (log-likelihood change)\n            if np.all(np.abs(old_w - self.weights_) < self.tol) and np.all(np.abs(old_m - self.means_) < self.tol) and np.all(np.abs(old_c - self.covariances_) < self.tol):\n                break\n\n    def predict(self, X):\n        # Compute the responsibilities for each sample\n        n_samples = len(X)\n        r = np.zeros((self.n_components, n_samples))\n\n        for i in range(self.n_components):\n            r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, self.means_[i], self.covariances_[i])\n\n        # Assign each sample to the component with the highest responsibility\n        return np.argmax(r, axis=0)\n","metadata":{"id":"Vafj21WYiM-v"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scipy","metadata":{"id":"nNErw0WPoH3R","outputId":"60f035d8-1127-455a-817c-703b94c34f2c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n# from scipy import random\nfrom numpy import random\n\nfrom matplotlib.patches import Ellipse\nimport matplotlib.transforms as transforms\nfrom scipy.stats import multivariate_normal\n\nclass GMM():\n    def __init__(self, k, dim, init_mu=None, init_sigma=None, init_pi=None, colors=None):\n        '''\n        Define a model with known number of clusters and dimensions.\n        input:\n            - k: Number of Gaussian clusters\n            - dim: Dimension\n            - init_mu: initial value of mean of clusters (k, dim)\n                       (default) random from uniform[-10, 10]\n            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)\n                          (default) Identity matrix for each cluster\n            - init_pi: initial value of cluster weights (k,)\n                       (default) equal value to all cluster i.e. 1/k\n            - colors: Color valu for plotting each cluster (k, 3)\n                      (default) random from uniform[0, 1]\n        '''\n        self.k = k\n        self.dim = dim\n        if(init_mu is None):\n            init_mu = random.rand(k, dim)*20 - 10\n        self.mu = init_mu\n        if(init_sigma is None):\n            init_sigma = np.zeros((k, dim, dim))\n            for i in range(k):\n                init_sigma[i] = np.eye(dim)\n        self.sigma = init_sigma\n        if(init_pi is None):\n            init_pi = np.ones(self.k)/self.k\n        self.pi = init_pi\n        if(colors is None):\n            colors = random.rand(k, 3)\n        self.colors = colors\n\n    def init_em(self, X):\n        '''\n        Initialization for EM algorithm.\n        input:\n            - X: data (batch_size, dim)\n        '''\n        self.data = X\n        self.num_points = X.shape[0]\n        self.z = np.zeros((self.num_points, self.k))\n\n    def e_step(self):\n        '''\n        E-step of EM algorithm.\n        '''\n        for i in range(self.k):\n            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])\n        self.z /= self.z.sum(axis=1, keepdims=True)\n\n    def m_step(self):\n        '''\n        M-step of EM algorithm.\n        '''\n        sum_z = self.z.sum(axis=0)\n        self.pi = sum_z / self.num_points\n        self.mu = np.matmul(self.z.T, self.data)\n        self.mu /= sum_z[:, None]\n        for i in range(self.k):\n            j = np.expand_dims(self.data, axis=1) - self.mu[i]\n            s = np.matmul(j.transpose([0, 2, 1]), j)\n            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), self.z[:, i] )\n            self.sigma[i] /= sum_z[i]\n\n    def log_likelihood(self, X):\n        '''\n        Compute the log-likelihood of X under current parameters\n        input:\n            - X: Data (batch_size, dim)\n        output:\n            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))\n        '''\n        ll = []\n        for d in X:\n            tot = 0\n            for i in range(self.k):\n                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])\n            ll.append(np.log(tot))\n        return np.sum(ll)\n\n    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):\n        '''\n        Utility function to plot one Gaussian from mean and covariance.\n        '''\n        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n        ell_radius_x = np.sqrt(1 + pearson)\n        ell_radius_y = np.sqrt(1 - pearson)\n        ellipse = Ellipse((0, 0),\n            width=ell_radius_x * 2,\n            height=ell_radius_y * 2,\n            facecolor=facecolor,\n            **kwargs)\n        scale_x = np.sqrt(cov[0, 0]) * n_std\n        mean_x = mean[0]\n        scale_y = np.sqrt(cov[1, 1]) * n_std\n        mean_y = mean[1]\n        transf = transforms.Affine2D() \\\n            .rotate_deg(45) \\\n            .scale(scale_x, scale_y) \\\n            .translate(mean_x, mean_y)\n        ellipse.set_transform(transf + ax.transData)\n        return ax.add_patch(ellipse)\n\n    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):\n        '''\n        Function to draw the Gaussians.\n        Note: Only for two-dimensionl dataset\n        '''\n        if(self.dim != 2):\n            print(\"Drawing available only for 2D case.\")\n            return\n        for i in range(self.k):\n            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)","metadata":{"id":"NhnYPz4Nn9wN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training the GMMo using EM\ngmmo = GMM(3, 2)\n\n# Initialize EM algo with data\ngmmo.init_em(X)\nnum_iters = 30\n# Saving log-likelihood\nlog_likelihood = [gmmo.log_likelihood(X)]\n# plotting\n# plot(\"Iteration:  0\")\nfor e in range(num_iters):\n    # E-step\n    gmmo.e_step()\n    # M-step\n    gmmo.m_step()\n    # Computing log-likelihood\n    log_likelihood.append(gmmo.log_likelihood(X))\n    print(\"Iteration: {}, log-likelihood: {:.4f}\".format(e+1, log_likelihood[-1]))\n    # plotting\n    # plot(title=\"Iteration: \" + str(e+1))","metadata":{"id":"ZxQTVnC0oFEG","outputId":"e3f821ef-bfdd-4f35-bba8-c837d5659013"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\n\n\nclass GMMf(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6, max_iter=100, verbose=False):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.verbose = verbose\n\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n\n    def fit(self, X):\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.means_ = np.random.rand(self.n_components, n_features) * 20 - 10\n        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n        self.weights_ = np.ones(self.n_components) / self.n_components\n\n        r = np.zeros((self.n_components, n_samples))\n\n        prev_log_likelihood = None\n\n        for iteration in range(self.max_iter):\n            # E-step\n            for i in range(self.n_components):\n                r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, mean=self.means_[i], cov=self.covariances_[i])\n\n            r /= r.sum(axis=0, keepdims=True) + 1e-10  # Normalize responsibilities\n\n            # M-step\n            Nk = r.sum(axis=1)  # shape (n_components,)\n\n            self.weights_ = Nk / n_samples\n\n            self.means_ = (r @ X) / Nk[:, np.newaxis]\n\n            for i in range(self.n_components):\n                diff = X - self.means_[i]\n                self.covariances_[i] = (r[i, :, np.newaxis, np.newaxis] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk[i]\n                self.covariances_[i] += self.reg_covar * np.eye(n_features)\n\n            # Check convergence using log-likelihood\n            log_likelihood = self.log_likelihood(X)\n            if self.verbose:\n                print(f\"Iteration {iteration}: log-likelihood = {log_likelihood:.6f}\")\n\n            if prev_log_likelihood is not None and abs(log_likelihood - prev_log_likelihood) < self.tol:\n                if self.verbose:\n                    print(f\"Converged at iteration {iteration}\")\n                break\n            prev_log_likelihood = log_likelihood\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        r = np.zeros((self.n_components, n_samples))\n\n        for i in range(self.n_components):\n            r[i, :] = self.weights_[i] * multivariate_normal.pdf(X, mean=self.means_[i], cov=self.covariances_[i])\n\n        return np.argmax(r, axis=0)\n\n    def log_likelihood(self, X):\n        '''\n        Compute the log-likelihood of X under current parameters.\n        '''\n        n_samples = X.shape[0]\n        total_likelihood = np.zeros(n_samples)\n\n        for i in range(self.n_components):\n            total_likelihood += self.weights_[i] * multivariate_normal.pdf(X, mean=self.means_[i], cov=self.covariances_[i])\n\n        log_likelihood = np.sum(np.log(total_likelihood + 1e-10))  # Add epsilon to avoid log(0)\n        return log_likelihood\n","metadata":{"id":"Cs5OK60tonIa"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ZZIx6qfasdJj"},"outputs":[],"execution_count":null}]}