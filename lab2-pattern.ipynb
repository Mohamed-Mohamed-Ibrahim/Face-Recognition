{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":847361,"sourceType":"datasetVersion","datasetId":244146}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision.transforms as transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:12:33.189255Z","iopub.execute_input":"2025-04-30T12:12:33.189554Z","iopub.status.idle":"2025-04-30T12:12:43.177391Z","shell.execute_reply.started":"2025-04-30T12:12:33.189536Z","shell.execute_reply":"2025-04-30T12:12:43.176395Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **Create Data Matrix & Labels**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\n# Check if the directory exists\ninput_path = \"/kaggle/input/\"\n# if not os.path.exists(input_path):\n#     print(\"Error: Dataset directory not found. Check the path or add the dataset.\")\n#     # Try listing available datasets (for Kaggle)\n#     print(\"Available datasets in /kaggle/input/:\")\n#     print(os.listdir(\"/kaggle/input/\"))\n# else:\nindex = 0\ndata = np.zeros((400, 10304))\nlabels = np.zeros(400)\n\nfor folder_name in os.listdir(input_path):\n    if folder_name == \"README\":\n        continue\n\n    root_path = os.path.join(input_path, folder_name)\n    \n    for img in os.listdir(root_path):\n        img_path = os.path.join(root_path, img)\n        img = np.asarray(Image.open(img_path))\n        img = img / 255.0\n        \n        flatten_img = np.ravel(img)\n        data[index] = flatten_img\n        \n        labels[index] = int(folder_name[1:])  # Assumes folder names are like 's1', 's2', etc.\n        index += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:48:33.163727Z","iopub.execute_input":"2025-04-30T12:48:33.164056Z","iopub.status.idle":"2025-04-30T12:48:34.180904Z","shell.execute_reply.started":"2025-04-30T12:48:33.164034Z","shell.execute_reply":"2025-04-30T12:48:34.179816Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## **Split Data Into Train and Test**","metadata":{}},{"cell_type":"code","source":"X_train = train_data   = data[::2]\ny_train = train_labels = labels[::2]\ny_train = y_train.astype(int)\nX_test = test_data   = data[1::2]\ny_test = test_labels = labels[1::2]\ny_test = y_test.astype(int)\nprint(np.max(y_train))\nnp.max(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:50:38.363930Z","iopub.execute_input":"2025-04-30T12:50:38.364291Z","iopub.status.idle":"2025-04-30T12:50:38.372576Z","shell.execute_reply.started":"2025-04-30T12:50:38.364267Z","shell.execute_reply":"2025-04-30T12:50:38.371626Z"}},"outputs":[{"name":"stdout","text":"40\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"40"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"## **Create Custom Dataset**","metadata":{}},{"cell_type":"code","source":"class FaceDataset(Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img = self.data[idx]\n        img.resize((112, 92))\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).float()\n            img = img.unsqueeze(0)\n        return img, img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:50:41.023820Z","iopub.execute_input":"2025-04-30T12:50:41.024103Z","iopub.status.idle":"2025-04-30T12:50:41.030045Z","shell.execute_reply.started":"2025-04-30T12:50:41.024084Z","shell.execute_reply":"2025-04-30T12:50:41.029008Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## **Create Dataloaders**","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(p = 0.5),\n    transforms.ColorJitter(brightness = 0.2, contrast = 0.2),\n    transforms.ToTensor(),\n])\n\ntrain_idx, val_idx, y_train, y_test = train_test_split(np.arange(len(train_data)), np.arange(len(train_labels)), test_size = 0.2, random_state = 42)\ntrain_images = train_data[train_idx]\nval_images   = train_data[val_idx]\ny_train      = train_labels[y_train]\ny_test       = train_labels[y_test]\n\n\ntrain_dataset = FaceDataset(train_images, transform = train_transform)\nval_dataset   = FaceDataset(val_images)\n\nbatch_size   = 4  # You can adjust this\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nval_loader   = DataLoader(val_dataset, batch_size   = batch_size, shuffle = False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:49:28.239316Z","iopub.execute_input":"2025-04-30T12:49:28.239617Z","iopub.status.idle":"2025-04-30T12:49:28.250999Z","shell.execute_reply.started":"2025-04-30T12:49:28.239597Z","shell.execute_reply":"2025-04-30T12:49:28.249968Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=40) \nprint(f\"Original train shape: {X_train.shape}\")\nprint(f\"Original test shape: {X_test.shape}\")\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nprint(f\"PCA-reduced train shape: {X_train.shape}\")\nprint(f\"PCA-reduced test shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:50:44.067661Z","iopub.execute_input":"2025-04-30T12:50:44.067952Z","iopub.status.idle":"2025-04-30T12:50:44.397902Z","shell.execute_reply.started":"2025-04-30T12:50:44.067932Z","shell.execute_reply":"2025-04-30T12:50:44.397142Z"}},"outputs":[{"name":"stdout","text":"Original train shape: (200, 10304)\nOriginal test shape: (200, 10304)\nPCA-reduced train shape: (200, 40)\nPCA-reduced test shape: (200, 40)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"print(y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:50:59.291924Z","iopub.execute_input":"2025-04-30T12:50:59.292270Z","iopub.status.idle":"2025-04-30T12:50:59.298356Z","shell.execute_reply.started":"2025-04-30T12:50:59.292239Z","shell.execute_reply":"2025-04-30T12:50:59.297236Z"}},"outputs":[{"name":"stdout","text":"[32 32 32 32 32 39 39 39 39 39 26 26 26 26 26 20 20 20 20 20 18 18 18 18\n 18 25 25 25 25 25 24 24 24 24 24 14 14 14 14 14 40 40 40 40 40 27 27 27\n 27 27 33 33 33 33 33  1  1  1  1  1 11 11 11 11 11 31 31 31 31 31 35 35\n 35 35 35 13 13 13 13 13 19 19 19 19 19 29 29 29 29 29 37 37 37 37 37 10\n 10 10 10 10  8  8  8  8  8  5  5  5  5  5  7  7  7  7  7 28 28 28 28 28\n  9  9  9  9  9 15 15 15 15 15 21 21 21 21 21  2  2  2  2  2  6  6  6  6\n  6 30 30 30 30 30  3  3  3  3  3 23 23 23 23 23  4  4  4  4  4 16 16 16\n 16 16 38 38 38 38 38 17 17 17 17 17 36 36 36 36 36 22 22 22 22 22 34 34\n 34 34 34 12 12 12 12 12] [32 32 32 32 32 39 39 39 39 39 26 26 26 26 26 20 20 20 20 20 18 18 18 18\n 18 25 25 25 25 25 24 24 24 24 24 14 14 14 14 14 40 40 40 40 40 27 27 27\n 27 27 33 33 33 33 33  1  1  1  1  1 11 11 11 11 11 31 31 31 31 31 35 35\n 35 35 35 13 13 13 13 13 19 19 19 19 19 29 29 29 29 29 37 37 37 37 37 10\n 10 10 10 10  8  8  8  8  8  5  5  5  5  5  7  7  7  7  7 28 28 28 28 28\n  9  9  9  9  9 15 15 15 15 15 21 21 21 21 21  2  2  2  2  2  6  6  6  6\n  6 30 30 30 30 30  3  3  3  3  3 23 23 23 23 23  4  4  4  4  4 16 16 16\n 16 16 38 38 38 38 38 17 17 17 17 17 36 36 36 36 36 22 22 22 22 22 34 34\n 34 34 34 12 12 12 12 12]\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"# **GMM**","metadata":{}},{"cell_type":"markdown","source":"## Main Class","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp  # Added missing import\n\n\nclass GMM(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-06, max_iter=100, verbose=False, n_init=10, early_stop=1):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.verbose = verbose\n        self.n_init = n_init\n        self.early_stop = early_stop\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n        \n        self.converged_ = False\n\n    def fit(self, X):\n        best_m = None\n        best_c = None\n        best_w = None\n        best_log_likelihood = -np.inf\n\n        for init in range(self.n_init):\n            if self.verbose:\n                print(f\"Model #: {init}\")\n            self.__fit(X)\n            ll = self.score(X)\n            if ll > best_log_likelihood:\n                best_log_likelihood = ll\n                best_m = self.means_\n                best_c = self.covariances_\n                best_w = self.weights_\n            self.means_ = None\n            self.covariances_ = None\n            self.weights_ = None\n        self.means_ = best_m\n        self.covariances_ = best_c\n        self.weights_ = best_w\n        return self\n\n    def __fit(self, X):\n        n_samples, n_features = X.shape\n        \n        # Initialize means with random samples - ensure they're truly different from each other\n        idx = np.random.choice(n_samples, self.n_components, replace=False)\n        self.means_ = X[idx].copy()\n        \n        # Add small random perturbations to ensure uniqueness\n        self.means_ += np.random.normal(0, 0.01, self.means_.shape)\n        \n        # Initialize covariances with data variance - ensure they're different for each component\n        base_cov = np.eye(n_features) * np.var(X, axis=0)\n        self.covariances_ = np.array([\n            base_cov * (0.5 + np.random.rand()) + self.reg_covar \n            for _ in range(self.n_components)\n        ])\n        \n        # Initialize with slightly different weights to break symmetry\n        raw_weights = np.random.rand(self.n_components) + 0.5\n        self.weights_ = raw_weights / np.sum(raw_weights)\n\n        log_likelihood_old = -np.inf\n        patience = self.early_stop\n        no_improvement_count = 0\n\n        # Debug\n        if self.verbose:\n            print(f\"Initial weights: {self.weights_}\")\n            print(f\"Initial means shape: {self.means_.shape}\")\n            print(f\"Initial covariances shape: {self.covariances_.shape}\")\n\n        for iteration in range(self.max_iter):\n            if self.verbose and iteration % 10 == 0:\n                print(f\"Iteration: {iteration} -> log_likelihood_old -> {log_likelihood_old}\")\n            \n            # E-step: calculate responsibilities - vectorized version\n            weighted_log_prob = np.zeros((n_samples, self.n_components))\n            \n            for k in range(self.n_components):\n                # Try-except to catch singular matrix errors\n                try:\n                    log_pdf = multivariate_normal.logpdf(\n                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"Warning in component {k}: {e}\")\n                    # Fallback to a more robust approach\n                    cov_regularized = self.covariances_[k] + np.eye(n_features) * self.reg_covar * 10\n                    log_pdf = multivariate_normal.logpdf(\n                        X, mean=self.means_[k], cov=cov_regularized, allow_singular=True)\n                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n            \n            # Normalize log probabilities for numerical stability\n            log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n            log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n            resp = np.exp(log_resp)\n            \n            # Ensure no numerical issues\n            resp = np.maximum(resp, np.finfo(resp.dtype).tiny)\n            row_sums = resp.sum(axis=1, keepdims=True)\n            resp = resp / row_sums  # Ensure each row sums to 1.0\n            \n            # Debug\n            if self.verbose and iteration % 10 == 0:\n                component_resp_sums = resp.sum(axis=0)\n                print(f\"Component responsibility sums: {component_resp_sums}\")\n                print(f\"Min resp: {resp.min()}, Max resp: {resp.max()}\")\n            \n            # M-step: update parameters\n            for k in range(self.n_components):\n                resp_sum = np.sum(resp[:, k])\n                \n                if self.verbose and iteration == 0:\n                    print(f\"resp_sum for component {k}: {resp_sum}\")\n                \n                if resp_sum > 1e-6:  # Prevent division by very small numbers\n                    # Update weights\n                    self.weights_[k] = resp_sum / n_samples\n                    \n                    # Update means\n                    weighted_sum = np.sum(resp[:, k, np.newaxis] * X, axis=0)\n                    self.means_[k] = weighted_sum / resp_sum\n                    \n                    # Update covariances with careful handling\n                    diff = X - self.means_[k]\n                    \n                    # Method 1: Direct calculation\n                    weighted_diff = resp[:, k, np.newaxis] * diff\n                    cov = np.dot(weighted_diff.T, diff) / resp_sum\n                    \n                    # Ensure positive definiteness\n                    min_eig = np.min(np.linalg.eigvalsh(cov))\n                    if min_eig < self.reg_covar:\n                        cov.flat[::n_features + 1] += (self.reg_covar - min_eig)\n                    \n                    self.covariances_[k] = cov\n                else:\n                    # Handle the degenerate case - reinitialize this component\n                    if self.verbose:\n                        print(f\"Reinitializing component {k} due to small responsibility sum\")\n                    self.weights_[k] = 1e-3  # Small but non-zero weight\n                    self.means_[k] = X[np.random.choice(n_samples)] + np.random.normal(0, 0.01, n_features)\n                    self.covariances_[k] = np.eye(n_features) * np.var(X, axis=0) * np.random.rand() + self.reg_covar\n\n            # Normalize weights to sum to 1\n            self.weights_ = self.weights_ / np.sum(self.weights_)\n            \n            # Check for convergence\n            try:\n                current_log_likelihood = self.score(X)\n                if self.verbose and iteration % 10 == 0:\n                    print(f\"Log-likelihood: {current_log_likelihood}\")\n                    \n                if np.abs(current_log_likelihood - log_likelihood_old) < self.tol:\n                    no_improvement_count += 1\n                else:\n                    no_improvement_count = 0\n\n                if no_improvement_count >= patience:\n                    self.converged_ = True\n                    if self.verbose:\n                        print(f\"Early stopping at iteration {iteration}\")\n                    break\n                    \n                log_likelihood_old = current_log_likelihood\n            except Exception as e:\n                if self.verbose:\n                    print(f\"Error in convergence check: {e}\")\n                # Continue anyway with adjusted parameters\n\n    def predict(self, X):\n        if self.means_ is None:\n            raise ValueError(\"Model not fitted yet.\")\n            \n        n_samples = len(X)\n        log_responsibilities = np.zeros((n_samples, self.n_components))\n\n        for k in range(self.n_components):\n            try:\n                # Use log space for numerical stability\n                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n            except Exception as e:\n                # Handle potential numerical issues\n                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n        \n        # Convert log responsibilities to probabilities and find the max\n        return np.argmax(log_responsibilities, axis=1)\n\n    def score(self, X):\n        if self.means_ is None:\n            raise ValueError(\"Model not fitted yet.\")\n            \n        n_samples = X.shape[0]\n        log_prob = np.zeros((n_samples, self.n_components))\n        \n        for k in range(self.n_components):\n            try:\n                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n            except Exception as e:\n                # Handle potential numerical issues\n                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n        \n        # Use logsumexp for numerical stability\n        return np.sum(logsumexp(log_prob, axis=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:49:33.191041Z","iopub.execute_input":"2025-04-30T12:49:33.191371Z","iopub.status.idle":"2025-04-30T12:49:33.219260Z","shell.execute_reply.started":"2025-04-30T12:49:33.191348Z","shell.execute_reply":"2025-04-30T12:49:33.218251Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom scipy.special import logsumexp\n\nimport numpy as np\n\ndef clusters_meaning(X, y, n_pred_clusters, n_classes):\n    # print(len(np.unique(X)), len(np.unique(y)))\n    cluster_map = np.zeros((n_pred_clusters, n_classes), dtype=int)\n    # print(X, y)\n    for i in range(len(X)):\n        cluster_map[X[i], (y[i]-1)] += 1\n\n    cluster_meanings = np.zeros(n_pred_clusters, dtype=int)\n    for i, row in enumerate(cluster_map):\n        cluster_meanings[i] = np.argmax(row)\n    return cluster_meanings\n\n# Ensure X, y are already defined\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nn_components_list = [20, 40, 60]\n# n_components_list = [40]\n\nfor n in n_components_list:\n    print(f\"\\n===== n_components = {n} =====\")\n\n    # ▶️ Custom GMM\n    gmm = GMM(n_components=n, max_iter=50, tol=1e-3, n_init=3, verbose=False)\n    gmm.fit(X_train)\n    y_pred = gmm.predict(X_test)\n    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n    \n    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n    \n    print(\"\\n[GMM]\")\n    print(f\"Accuracy: {acc_sklearn:.4f}\")\n    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n\n    # ▶️ Sklearn GaussianMixture\n    gmm = GaussianMixture(n_components=n, max_iter=50, tol=1e-3, n_init=3, random_state=42)\n    gmm.fit(X_train)\n    y_pred = gmm.predict(X_test)\n    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n    \n    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n\n    print(\"\\n[Sklearn GMM]\")\n    print(f\"Accuracy: {acc_sklearn:.4f}\")\n    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n\n    # ▶️ KNN\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    y_pred_knn = knn.predict(X_test)\n\n    acc_knn = accuracy_score(y_test, y_pred_knn)\n    f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n\n    print(\"\\n[KNN]\")\n    print(f\"Accuracy: {acc_knn:.4f}\")\n    print(f\"F1 Score:  {f1_knn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_knn))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T12:51:58.651941Z","iopub.execute_input":"2025-04-30T12:51:58.652324Z","iopub.status.idle":"2025-04-30T12:52:04.000758Z","shell.execute_reply.started":"2025-04-30T12:51:58.652298Z","shell.execute_reply":"2025-04-30T12:52:03.999880Z"}},"outputs":[{"name":"stdout","text":"\n===== n_components = 20 =====\n\n[GMM]\nAccuracy: 0.0050\nF1 Score:  0.0030\n[[0 0 0 ... 0 0 0]\n [2 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n[Sklearn GMM]\nAccuracy: 0.0000\nF1 Score:  0.0000\n[[0 0 0 ... 0 0 0]\n [5 0 0 ... 0 0 0]\n [0 4 0 ... 0 0 0]\n ...\n [0 1 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n[KNN]\nAccuracy: 0.5550\nF1 Score:  0.4893\n[[2 1 0 ... 0 0 0]\n [0 5 0 ... 0 0 0]\n [0 0 4 ... 0 0 0]\n ...\n [0 0 0 ... 5 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n===== n_components = 40 =====\n\n[GMM]\nAccuracy: 0.0050\nF1 Score:  0.0041\n[[0 0 0 ... 0 0 0]\n [2 0 0 ... 0 0 0]\n [0 4 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 0 0 0]]\n\n[Sklearn GMM]\nAccuracy: 0.0000\nF1 Score:  0.0000\n[[0 0 0 ... 0 0 0]\n [3 0 0 ... 0 0 0]\n [0 5 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n[KNN]\nAccuracy: 0.3050\nF1 Score:  0.2410\n[[2 1 0 ... 0 0 0]\n [0 5 0 ... 0 0 0]\n [0 0 4 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n===== n_components = 60 =====\n\n[GMM]\nAccuracy: 0.0050\nF1 Score:  0.0063\n[[0 0 0 ... 0 0 0]\n [5 0 0 ... 0 0 0]\n [0 3 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 0 1 0]]\n\n[Sklearn GMM]\nAccuracy: 0.0000\nF1 Score:  0.0000\n[[0 0 0 ... 0 0 0]\n [3 0 0 ... 0 0 0]\n [0 5 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 5 0 0]\n [0 0 0 ... 0 3 0]]\n\n[KNN]\nAccuracy: 0.1500\nF1 Score:  0.0968\n[[2 1 0 ... 0 0 0]\n [0 5 0 ... 0 0 0]\n [0 0 5 ... 0 0 0]\n ...\n [0 0 1 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 4 ... 0 0 0]]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Auto Encoder","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}