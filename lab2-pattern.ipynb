{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf17bef",
   "metadata": {
    "papermill": {
     "duration": 0.003944,
     "end_time": "2025-04-30T11:24:34.829753",
     "exception": false,
     "start_time": "2025-04-30T11:24:34.825809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4490effb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:34.837912Z",
     "iopub.status.busy": "2025-04-30T11:24:34.837605Z",
     "iopub.status.idle": "2025-04-30T11:24:46.310143Z",
     "shell.execute_reply": "2025-04-30T11:24:46.309423Z"
    },
    "papermill": {
     "duration": 11.478396,
     "end_time": "2025-04-30T11:24:46.311763",
     "exception": false,
     "start_time": "2025-04-30T11:24:34.833367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424a044",
   "metadata": {
    "papermill": {
     "duration": 0.003076,
     "end_time": "2025-04-30T11:24:46.318500",
     "exception": false,
     "start_time": "2025-04-30T11:24:46.315424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Create Data Matrix & Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d3fa58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:46.326775Z",
     "iopub.status.busy": "2025-04-30T11:24:46.325987Z",
     "iopub.status.idle": "2025-04-30T11:24:48.496031Z",
     "shell.execute_reply": "2025-04-30T11:24:48.495114Z"
    },
    "papermill": {
     "duration": 2.175795,
     "end_time": "2025-04-30T11:24:48.497663",
     "exception": false,
     "start_time": "2025-04-30T11:24:46.321868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Check if the directory exists\n",
    "input_path = \"/kaggle/input/\"\n",
    "# if not os.path.exists(input_path):\n",
    "#     print(\"Error: Dataset directory not found. Check the path or add the dataset.\")\n",
    "#     # Try listing available datasets (for Kaggle)\n",
    "#     print(\"Available datasets in /kaggle/input/:\")\n",
    "#     print(os.listdir(\"/kaggle/input/\"))\n",
    "# else:\n",
    "index = 0\n",
    "data = np.zeros((400, 10304))\n",
    "labels = np.zeros(400)\n",
    "\n",
    "for folder_name in os.listdir(input_path):\n",
    "    if folder_name == \"README\":\n",
    "        continue\n",
    "\n",
    "    root_path = os.path.join(input_path, folder_name)\n",
    "    \n",
    "    for img in os.listdir(root_path):\n",
    "        img_path = os.path.join(root_path, img)\n",
    "        img = np.asarray(Image.open(img_path))\n",
    "        img = img / 255.0\n",
    "        \n",
    "        flatten_img = np.ravel(img)\n",
    "        data[index] = flatten_img\n",
    "        \n",
    "        labels[index] = int(folder_name[1:])  # Assumes folder names are like 's1', 's2', etc.\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca7b59",
   "metadata": {
    "papermill": {
     "duration": 0.004307,
     "end_time": "2025-04-30T11:24:48.505546",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.501239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Split Data Into Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d1f0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:48.513552Z",
     "iopub.status.busy": "2025-04-30T11:24:48.512873Z",
     "iopub.status.idle": "2025-04-30T11:24:48.517394Z",
     "shell.execute_reply": "2025-04-30T11:24:48.516591Z"
    },
    "papermill": {
     "duration": 0.010022,
     "end_time": "2025-04-30T11:24:48.518743",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.508721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X =train_data   = data[::2]\n",
    "y = train_labels = labels[::2]\n",
    "y = y.astype(int)\n",
    "\n",
    "test_data   = data[1::2]\n",
    "test_labels = labels[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382094b",
   "metadata": {
    "papermill": {
     "duration": 0.00306,
     "end_time": "2025-04-30T11:24:48.525228",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.522168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Create Custom Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc1b4d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:48.533191Z",
     "iopub.status.busy": "2025-04-30T11:24:48.532870Z",
     "iopub.status.idle": "2025-04-30T11:24:48.538480Z",
     "shell.execute_reply": "2025-04-30T11:24:48.537771Z"
    },
    "papermill": {
     "duration": 0.011381,
     "end_time": "2025-04-30T11:24:48.539835",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.528454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        img.resize((112, 92))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img).float()\n",
    "            img = img.unsqueeze(0)\n",
    "        return img, img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece218e",
   "metadata": {
    "papermill": {
     "duration": 0.002985,
     "end_time": "2025-04-30T11:24:48.546170",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.543185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Create Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84db97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:48.553678Z",
     "iopub.status.busy": "2025-04-30T11:24:48.553354Z",
     "iopub.status.idle": "2025-04-30T11:24:48.573728Z",
     "shell.execute_reply": "2025-04-30T11:24:48.572801Z"
    },
    "papermill": {
     "duration": 0.025957,
     "end_time": "2025-04-30T11:24:48.575346",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.549389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.ColorJitter(brightness = 0.2, contrast = 0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_idx, val_idx, y_train, y_test = train_test_split(np.arange(len(train_data)), np.arange(len(train_labels)), test_size = 0.2, random_state = 42)\n",
    "train_images = train_data[train_idx]\n",
    "val_images   = train_data[val_idx]\n",
    "y_train      = train_labels[y_train]\n",
    "y_test       = train_labels[y_test]\n",
    "\n",
    "\n",
    "train_dataset = FaceDataset(train_images, transform = train_transform)\n",
    "val_dataset   = FaceDataset(val_images)\n",
    "\n",
    "batch_size   = 4  # You can adjust this\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size   = batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6822b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:48.583684Z",
     "iopub.status.busy": "2025-04-30T11:24:48.582917Z",
     "iopub.status.idle": "2025-04-30T11:24:49.122512Z",
     "shell.execute_reply": "2025-04-30T11:24:49.121733Z"
    },
    "papermill": {
     "duration": 0.545214,
     "end_time": "2025-04-30T11:24:49.124008",
     "exception": false,
     "start_time": "2025-04-30T11:24:48.578794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (200, 10304)\n",
      "PCA-reduced shape: (200, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=40) \n",
    "print(f\"Original shape: {X.shape}\")\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "print(f\"PCA-reduced shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a902",
   "metadata": {
    "papermill": {
     "duration": 0.005407,
     "end_time": "2025-04-30T11:24:49.135044",
     "exception": false,
     "start_time": "2025-04-30T11:24:49.129637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **GMM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c67ec",
   "metadata": {
    "papermill": {
     "duration": 0.005042,
     "end_time": "2025-04-30T11:24:49.145588",
     "exception": false,
     "start_time": "2025-04-30T11:24:49.140546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c4c93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:49.158272Z",
     "iopub.status.busy": "2025-04-30T11:24:49.157936Z",
     "iopub.status.idle": "2025-04-30T11:24:49.202487Z",
     "shell.execute_reply": "2025-04-30T11:24:49.201722Z"
    },
    "papermill": {
     "duration": 0.053367,
     "end_time": "2025-04-30T11:24:49.204379",
     "exception": false,
     "start_time": "2025-04-30T11:24:49.151012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp  # Added missing import\n",
    "\n",
    "\n",
    "class GMM(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-06, max_iter=100, verbose=False, n_init=10, early_stop=1):\n",
    "        self.n_components = n_components\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.n_init = n_init\n",
    "        self.early_stop = early_stop\n",
    "        self.means_ = None\n",
    "        self.covariances_ = None\n",
    "        self.weights_ = None\n",
    "        \n",
    "        self.converged_ = False\n",
    "\n",
    "    def fit(self, X):\n",
    "        best_m = None\n",
    "        best_c = None\n",
    "        best_w = None\n",
    "        best_log_likelihood = -np.inf\n",
    "\n",
    "        for init in range(self.n_init):\n",
    "            if self.verbose:\n",
    "                print(f\"Model #: {init}\")\n",
    "            self.__fit(X)\n",
    "            ll = self.score(X)\n",
    "            if ll > best_log_likelihood:\n",
    "                best_log_likelihood = ll\n",
    "                best_m = self.means_\n",
    "                best_c = self.covariances_\n",
    "                best_w = self.weights_\n",
    "            self.means_ = None\n",
    "            self.covariances_ = None\n",
    "            self.weights_ = None\n",
    "        self.means_ = best_m\n",
    "        self.covariances_ = best_c\n",
    "        self.weights_ = best_w\n",
    "        return self\n",
    "\n",
    "    def __fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize means with random samples - ensure they're truly different from each other\n",
    "        idx = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[idx].copy()\n",
    "        \n",
    "        # Add small random perturbations to ensure uniqueness\n",
    "        self.means_ += np.random.normal(0, 0.01, self.means_.shape)\n",
    "        \n",
    "        # Initialize covariances with data variance - ensure they're different for each component\n",
    "        base_cov = np.eye(n_features) * np.var(X, axis=0)\n",
    "        self.covariances_ = np.array([\n",
    "            base_cov * (0.5 + np.random.rand()) + self.reg_covar \n",
    "            for _ in range(self.n_components)\n",
    "        ])\n",
    "        \n",
    "        # Initialize with slightly different weights to break symmetry\n",
    "        raw_weights = np.random.rand(self.n_components) + 0.5\n",
    "        self.weights_ = raw_weights / np.sum(raw_weights)\n",
    "\n",
    "        log_likelihood_old = -np.inf\n",
    "        patience = self.early_stop\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        # Debug\n",
    "        if self.verbose:\n",
    "            print(f\"Initial weights: {self.weights_}\")\n",
    "            print(f\"Initial means shape: {self.means_.shape}\")\n",
    "            print(f\"Initial covariances shape: {self.covariances_.shape}\")\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            if self.verbose and iteration % 10 == 0:\n",
    "                print(f\"Iteration: {iteration} -> log_likelihood_old -> {log_likelihood_old}\")\n",
    "            \n",
    "            # E-step: calculate responsibilities - vectorized version\n",
    "            weighted_log_prob = np.zeros((n_samples, self.n_components))\n",
    "            \n",
    "            for k in range(self.n_components):\n",
    "                # Try-except to catch singular matrix errors\n",
    "                try:\n",
    "                    log_pdf = multivariate_normal.logpdf(\n",
    "                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n",
    "                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n",
    "                except Exception as e:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Warning in component {k}: {e}\")\n",
    "                    # Fallback to a more robust approach\n",
    "                    cov_regularized = self.covariances_[k] + np.eye(n_features) * self.reg_covar * 10\n",
    "                    log_pdf = multivariate_normal.logpdf(\n",
    "                        X, mean=self.means_[k], cov=cov_regularized, allow_singular=True)\n",
    "                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n",
    "            \n",
    "            # Normalize log probabilities for numerical stability\n",
    "            log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n",
    "            log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n",
    "            resp = np.exp(log_resp)\n",
    "            \n",
    "            # Ensure no numerical issues\n",
    "            resp = np.maximum(resp, np.finfo(resp.dtype).tiny)\n",
    "            row_sums = resp.sum(axis=1, keepdims=True)\n",
    "            resp = resp / row_sums  # Ensure each row sums to 1.0\n",
    "            \n",
    "            # Debug\n",
    "            if self.verbose and iteration % 10 == 0:\n",
    "                component_resp_sums = resp.sum(axis=0)\n",
    "                print(f\"Component responsibility sums: {component_resp_sums}\")\n",
    "                print(f\"Min resp: {resp.min()}, Max resp: {resp.max()}\")\n",
    "            \n",
    "            # M-step: update parameters\n",
    "            for k in range(self.n_components):\n",
    "                resp_sum = np.sum(resp[:, k])\n",
    "                \n",
    "                if self.verbose and iteration == 0:\n",
    "                    print(f\"resp_sum for component {k}: {resp_sum}\")\n",
    "                \n",
    "                if resp_sum > 1e-6:  # Prevent division by very small numbers\n",
    "                    # Update weights\n",
    "                    self.weights_[k] = resp_sum / n_samples\n",
    "                    \n",
    "                    # Update means\n",
    "                    weighted_sum = np.sum(resp[:, k, np.newaxis] * X, axis=0)\n",
    "                    self.means_[k] = weighted_sum / resp_sum\n",
    "                    \n",
    "                    # Update covariances with careful handling\n",
    "                    diff = X - self.means_[k]\n",
    "                    \n",
    "                    # Method 1: Direct calculation\n",
    "                    weighted_diff = resp[:, k, np.newaxis] * diff\n",
    "                    cov = np.dot(weighted_diff.T, diff) / resp_sum\n",
    "                    \n",
    "                    # Ensure positive definiteness\n",
    "                    min_eig = np.min(np.linalg.eigvalsh(cov))\n",
    "                    if min_eig < self.reg_covar:\n",
    "                        cov.flat[::n_features + 1] += (self.reg_covar - min_eig)\n",
    "                    \n",
    "                    self.covariances_[k] = cov\n",
    "                else:\n",
    "                    # Handle the degenerate case - reinitialize this component\n",
    "                    if self.verbose:\n",
    "                        print(f\"Reinitializing component {k} due to small responsibility sum\")\n",
    "                    self.weights_[k] = 1e-3  # Small but non-zero weight\n",
    "                    self.means_[k] = X[np.random.choice(n_samples)] + np.random.normal(0, 0.01, n_features)\n",
    "                    self.covariances_[k] = np.eye(n_features) * np.var(X, axis=0) * np.random.rand() + self.reg_covar\n",
    "\n",
    "            # Normalize weights to sum to 1\n",
    "            self.weights_ = self.weights_ / np.sum(self.weights_)\n",
    "            \n",
    "            # Check for convergence\n",
    "            try:\n",
    "                current_log_likelihood = self.score(X)\n",
    "                if self.verbose and iteration % 10 == 0:\n",
    "                    print(f\"Log-likelihood: {current_log_likelihood}\")\n",
    "                    \n",
    "                if np.abs(current_log_likelihood - log_likelihood_old) < self.tol:\n",
    "                    no_improvement_count += 1\n",
    "                else:\n",
    "                    no_improvement_count = 0\n",
    "\n",
    "                if no_improvement_count >= patience:\n",
    "                    self.converged_ = True\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at iteration {iteration}\")\n",
    "                    break\n",
    "                    \n",
    "                log_likelihood_old = current_log_likelihood\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"Error in convergence check: {e}\")\n",
    "                # Continue anyway with adjusted parameters\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.means_ is None:\n",
    "            raise ValueError(\"Model not fitted yet.\")\n",
    "            \n",
    "        n_samples = len(X)\n",
    "        log_responsibilities = np.zeros((n_samples, self.n_components))\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                # Use log space for numerical stability\n",
    "                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n",
    "            except Exception as e:\n",
    "                # Handle potential numerical issues\n",
    "                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n",
    "                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n",
    "        \n",
    "        # Convert log responsibilities to probabilities and find the max\n",
    "        return np.argmax(log_responsibilities, axis=1)\n",
    "\n",
    "    def score(self, X):\n",
    "        if self.means_ is None:\n",
    "            raise ValueError(\"Model not fitted yet.\")\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        log_prob = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n",
    "                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n",
    "            except Exception as e:\n",
    "                # Handle potential numerical issues\n",
    "                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n",
    "                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n",
    "                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n",
    "        \n",
    "        # Use logsumexp for numerical stability\n",
    "        return np.sum(logsumexp(log_prob, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5c3cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:24:49.214633Z",
     "iopub.status.busy": "2025-04-30T11:24:49.214324Z",
     "iopub.status.idle": "2025-04-30T11:24:54.195305Z",
     "shell.execute_reply": "2025-04-30T11:24:54.194443Z"
    },
    "papermill": {
     "duration": 4.98688,
     "end_time": "2025-04-30T11:24:54.197034",
     "exception": false,
     "start_time": "2025-04-30T11:24:49.210154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== n_components = 20 =====\n",
      "15 26\n",
      "\n",
      "[GMM]\n",
      "Accuracy: 0.4250\n",
      "F1 Score:  0.3976\n",
      "[[1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "15 26\n",
      "\n",
      "[Sklearn GMM]\n",
      "Accuracy: 0.5250\n",
      "F1 Score:  0.3692\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "[KNN]\n",
      "Accuracy: 0.3000\n",
      "F1 Score:  0.2618\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "===== n_components = 40 =====\n",
      "21 26\n",
      "\n",
      "[GMM]\n",
      "Accuracy: 0.6500\n",
      "F1 Score:  0.5724\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "25 26\n",
      "\n",
      "[Sklearn GMM]\n",
      "Accuracy: 0.8000\n",
      "F1 Score:  0.7575\n",
      "[[1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "[KNN]\n",
      "Accuracy: 0.0500\n",
      "F1 Score:  0.0370\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "===== n_components = 60 =====\n",
      "2 26\n",
      "\n",
      "[GMM]\n",
      "Accuracy: 0.1250\n",
      "F1 Score:  0.0168\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n",
      "27 26\n",
      "\n",
      "[Sklearn GMM]\n",
      "Accuracy: 0.8250\n",
      "F1 Score:  0.7987\n",
      "[[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "[KNN]\n",
      "Accuracy: 0.0250\n",
      "F1 Score:  0.0147\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def clusters_meaning(X, y, n_pred_clusters, n_classes):\n",
    "    print(len(np.unique(X)), len(np.unique(y)))\n",
    "    cluster_map = np.zeros((n_pred_clusters, n_classes), dtype=int)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        cluster_map[X[i], y[i]] += 1\n",
    "\n",
    "    cluster_meanings = np.zeros(n_pred_clusters, dtype=int)\n",
    "    for i, row in enumerate(cluster_map):\n",
    "        cluster_meanings[i] = np.argmax(row)\n",
    "    return cluster_meanings\n",
    "\n",
    "# Ensure X, y are already defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_components_list = [20, 40, 60]\n",
    "# n_components_list = [40]\n",
    "\n",
    "for n in n_components_list:\n",
    "    print(f\"\\n===== n_components = {n} =====\")\n",
    "\n",
    "    # ▶️ Custom GMM\n",
    "    gmm = GMM(n_components=n, max_iter=50, tol=1e-3, n_init=3, verbose=False)\n",
    "    gmm.fit(X_train)\n",
    "    y_pred = gmm.predict(X_test)\n",
    "    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n",
    "    \n",
    "    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n",
    "    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n",
    "    \n",
    "    print(\"\\n[GMM]\")\n",
    "    print(f\"Accuracy: {acc_sklearn:.4f}\")\n",
    "    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n",
    "\n",
    "    # ▶️ Sklearn GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=n, max_iter=50, tol=1e-3, n_init=3, random_state=42)\n",
    "    gmm.fit(X_train)\n",
    "    y_pred = gmm.predict(X_test)\n",
    "    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n",
    "    \n",
    "    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n",
    "    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n",
    "\n",
    "    print(\"\\n[Sklearn GMM]\")\n",
    "    print(f\"Accuracy: {acc_sklearn:.4f}\")\n",
    "    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n",
    "\n",
    "    # ▶️ KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "    acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "    f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
    "\n",
    "    print(\"\\n[KNN]\")\n",
    "    print(f\"Accuracy: {acc_knn:.4f}\")\n",
    "    print(f\"F1 Score:  {f1_knn:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9c89c",
   "metadata": {
    "papermill": {
     "duration": 0.006287,
     "end_time": "2025-04-30T11:24:54.210623",
     "exception": false,
     "start_time": "2025-04-30T11:24:54.204336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fa5f8",
   "metadata": {
    "papermill": {
     "duration": 0.006101,
     "end_time": "2025-04-30T11:24:54.223104",
     "exception": false,
     "start_time": "2025-04-30T11:24:54.217003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 244146,
     "sourceId": 847361,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.09583,
   "end_time": "2025-04-30T11:24:57.423443",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-30T11:24:30.327613",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
