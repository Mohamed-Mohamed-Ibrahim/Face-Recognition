{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":847361,"sourceType":"datasetVersion","datasetId":244146}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision.transforms as transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T07:43:19.060640Z","iopub.execute_input":"2025-04-30T07:43:19.060983Z","iopub.status.idle":"2025-04-30T07:43:26.820938Z","shell.execute_reply.started":"2025-04-30T07:43:19.060958Z","shell.execute_reply":"2025-04-30T07:43:26.819916Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **Create Data Matrix & Labels**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\n# Check if the directory exists\ninput_path = \"/kaggle/input/\"\n# if not os.path.exists(input_path):\n#     print(\"Error: Dataset directory not found. Check the path or add the dataset.\")\n#     # Try listing available datasets (for Kaggle)\n#     print(\"Available datasets in /kaggle/input/:\")\n#     print(os.listdir(\"/kaggle/input/\"))\n# else:\nindex = 0\ndata = np.zeros((400, 10304))\nlabels = np.zeros(400)\n\nfor folder_name in os.listdir(input_path):\n    if folder_name == \"README\":\n        continue\n\n    root_path = os.path.join(input_path, folder_name)\n    \n    for img in os.listdir(root_path):\n        img_path = os.path.join(root_path, img)\n        img = np.asarray(Image.open(img_path))\n        img = img / 255.0\n        \n        flatten_img = np.ravel(img)\n        data[index] = flatten_img\n        \n        labels[index] = int(folder_name[1:])  # Assumes folder names are like 's1', 's2', etc.\n        index += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T07:43:26.822858Z","iopub.execute_input":"2025-04-30T07:43:26.823523Z","iopub.status.idle":"2025-04-30T07:43:28.959491Z","shell.execute_reply.started":"2025-04-30T07:43:26.823487Z","shell.execute_reply":"2025-04-30T07:43:28.958315Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## **Split Data Into Train and Test**","metadata":{}},{"cell_type":"code","source":"X =train_data   = data[::2]\ny = train_labels = labels[::2]\ny = y.astype(int)\n\ntest_data   = data[1::2]\ntest_labels = labels[1::2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T08:08:46.757590Z","iopub.execute_input":"2025-04-30T08:08:46.758647Z","iopub.status.idle":"2025-04-30T08:08:46.764453Z","shell.execute_reply.started":"2025-04-30T08:08:46.758604Z","shell.execute_reply":"2025-04-30T08:08:46.763182Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## **Create Custom Dataset**","metadata":{}},{"cell_type":"code","source":"class FaceDataset(Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img = self.data[idx]\n        img.resize((112, 92))\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).float()\n            img = img.unsqueeze(0)\n        return img, img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T07:43:28.968830Z","iopub.execute_input":"2025-04-30T07:43:28.969137Z","iopub.status.idle":"2025-04-30T07:43:28.992659Z","shell.execute_reply.started":"2025-04-30T07:43:28.969113Z","shell.execute_reply":"2025-04-30T07:43:28.991706Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **Create Dataloaders**","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(p = 0.5),\n    transforms.ColorJitter(brightness = 0.2, contrast = 0.2),\n    transforms.ToTensor(),\n])\n\ntrain_idx, val_idx, y_train, y_test = train_test_split(np.arange(len(train_data)), np.arange(len(train_labels)), test_size = 0.2, random_state = 42)\ntrain_images = train_data[train_idx]\nval_images   = train_data[val_idx]\ny_train      = train_labels[y_train]\ny_test       = train_labels[y_test]\n\n\ntrain_dataset = FaceDataset(train_images, transform = train_transform)\nval_dataset   = FaceDataset(val_images)\n\nbatch_size   = 4  # You can adjust this\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nval_loader   = DataLoader(val_dataset, batch_size   = batch_size, shuffle = False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T06:51:20.978050Z","iopub.execute_input":"2025-04-30T06:51:20.978425Z","iopub.status.idle":"2025-04-30T06:51:20.995297Z","shell.execute_reply.started":"2025-04-30T06:51:20.978400Z","shell.execute_reply":"2025-04-30T06:51:20.993985Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=40) \nprint(f\"Original shape: {X.shape}\")\nX = pca.fit_transform(X)\n\nprint(f\"PCA-reduced shape: {X.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T08:08:49.791922Z","iopub.execute_input":"2025-04-30T08:08:49.792281Z","iopub.status.idle":"2025-04-30T08:08:50.073859Z","shell.execute_reply.started":"2025-04-30T08:08:49.792256Z","shell.execute_reply":"2025-04-30T08:08:50.073130Z"}},"outputs":[{"name":"stdout","text":"Original shape: (200, 10304)\nPCA-reduced shape: (200, 40)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# **GMM**","metadata":{}},{"cell_type":"markdown","source":"## Main Class","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp  # Added missing import\n\n\nclass GMM(BaseEstimator, RegressorMixin):\n    def __init__(self, n_components=1, tol=0.001, reg_covar=1e-06, max_iter=100, verbose=False, n_init=10, early_stop=1):\n        self.n_components = n_components\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.verbose = verbose\n        self.n_init = n_init\n        self.early_stop = early_stop\n        self.means_ = None\n        self.covariances_ = None\n        self.weights_ = None\n        \n        self.converged_ = False\n\n    def fit(self, X):\n        best_m = None\n        best_c = None\n        best_w = None\n        best_log_likelihood = -np.inf\n\n        for init in range(self.n_init):\n            if self.verbose:\n                print(f\"Model #: {init}\")\n            self.__fit(X)\n            ll = self.score(X)\n            if ll > best_log_likelihood:\n                best_log_likelihood = ll\n                best_m = self.means_\n                best_c = self.covariances_\n                best_w = self.weights_\n            self.means_ = None\n            self.covariances_ = None\n            self.weights_ = None\n        self.means_ = best_m\n        self.covariances_ = best_c\n        self.weights_ = best_w\n        return self\n\n    def __fit(self, X):\n        n_samples, n_features = X.shape\n        \n        # Initialize means with random samples - ensure they're truly different from each other\n        idx = np.random.choice(n_samples, self.n_components, replace=False)\n        self.means_ = X[idx].copy()\n        \n        # Add small random perturbations to ensure uniqueness\n        self.means_ += np.random.normal(0, 0.01, self.means_.shape)\n        \n        # Initialize covariances with data variance - ensure they're different for each component\n        base_cov = np.eye(n_features) * np.var(X, axis=0)\n        self.covariances_ = np.array([\n            base_cov * (0.5 + np.random.rand()) + self.reg_covar \n            for _ in range(self.n_components)\n        ])\n        \n        # Initialize with slightly different weights to break symmetry\n        raw_weights = np.random.rand(self.n_components) + 0.5\n        self.weights_ = raw_weights / np.sum(raw_weights)\n\n        log_likelihood_old = -np.inf\n        patience = self.early_stop\n        no_improvement_count = 0\n\n        # Debug\n        if self.verbose:\n            print(f\"Initial weights: {self.weights_}\")\n            print(f\"Initial means shape: {self.means_.shape}\")\n            print(f\"Initial covariances shape: {self.covariances_.shape}\")\n\n        for iteration in range(self.max_iter):\n            if self.verbose and iteration % 10 == 0:\n                print(f\"Iteration: {iteration} -> log_likelihood_old -> {log_likelihood_old}\")\n            \n            # E-step: calculate responsibilities - vectorized version\n            weighted_log_prob = np.zeros((n_samples, self.n_components))\n            \n            for k in range(self.n_components):\n                # Try-except to catch singular matrix errors\n                try:\n                    log_pdf = multivariate_normal.logpdf(\n                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"Warning in component {k}: {e}\")\n                    # Fallback to a more robust approach\n                    cov_regularized = self.covariances_[k] + np.eye(n_features) * self.reg_covar * 10\n                    log_pdf = multivariate_normal.logpdf(\n                        X, mean=self.means_[k], cov=cov_regularized, allow_singular=True)\n                    weighted_log_prob[:, k] = np.log(self.weights_[k]) + log_pdf\n            \n            # Normalize log probabilities for numerical stability\n            log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n            log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n            resp = np.exp(log_resp)\n            \n            # Ensure no numerical issues\n            resp = np.maximum(resp, np.finfo(resp.dtype).tiny)\n            row_sums = resp.sum(axis=1, keepdims=True)\n            resp = resp / row_sums  # Ensure each row sums to 1.0\n            \n            # Debug\n            if self.verbose and iteration % 10 == 0:\n                component_resp_sums = resp.sum(axis=0)\n                print(f\"Component responsibility sums: {component_resp_sums}\")\n                print(f\"Min resp: {resp.min()}, Max resp: {resp.max()}\")\n            \n            # M-step: update parameters\n            for k in range(self.n_components):\n                resp_sum = np.sum(resp[:, k])\n                \n                if self.verbose and iteration == 0:\n                    print(f\"resp_sum for component {k}: {resp_sum}\")\n                \n                if resp_sum > 1e-6:  # Prevent division by very small numbers\n                    # Update weights\n                    self.weights_[k] = resp_sum / n_samples\n                    \n                    # Update means\n                    weighted_sum = np.sum(resp[:, k, np.newaxis] * X, axis=0)\n                    self.means_[k] = weighted_sum / resp_sum\n                    \n                    # Update covariances with careful handling\n                    diff = X - self.means_[k]\n                    \n                    # Method 1: Direct calculation\n                    weighted_diff = resp[:, k, np.newaxis] * diff\n                    cov = np.dot(weighted_diff.T, diff) / resp_sum\n                    \n                    # Ensure positive definiteness\n                    min_eig = np.min(np.linalg.eigvalsh(cov))\n                    if min_eig < self.reg_covar:\n                        cov.flat[::n_features + 1] += (self.reg_covar - min_eig)\n                    \n                    self.covariances_[k] = cov\n                else:\n                    # Handle the degenerate case - reinitialize this component\n                    if self.verbose:\n                        print(f\"Reinitializing component {k} due to small responsibility sum\")\n                    self.weights_[k] = 1e-3  # Small but non-zero weight\n                    self.means_[k] = X[np.random.choice(n_samples)] + np.random.normal(0, 0.01, n_features)\n                    self.covariances_[k] = np.eye(n_features) * np.var(X, axis=0) * np.random.rand() + self.reg_covar\n\n            # Normalize weights to sum to 1\n            self.weights_ = self.weights_ / np.sum(self.weights_)\n            \n            # Check for convergence\n            try:\n                current_log_likelihood = self.score(X)\n                if self.verbose and iteration % 10 == 0:\n                    print(f\"Log-likelihood: {current_log_likelihood}\")\n                    \n                if np.abs(current_log_likelihood - log_likelihood_old) < self.tol:\n                    no_improvement_count += 1\n                else:\n                    no_improvement_count = 0\n\n                if no_improvement_count >= patience:\n                    self.converged_ = True\n                    if self.verbose:\n                        print(f\"Early stopping at iteration {iteration}\")\n                    break\n                    \n                log_likelihood_old = current_log_likelihood\n            except Exception as e:\n                if self.verbose:\n                    print(f\"Error in convergence check: {e}\")\n                # Continue anyway with adjusted parameters\n\n    def predict(self, X):\n        if self.means_ is None:\n            raise ValueError(\"Model not fitted yet.\")\n            \n        n_samples = len(X)\n        log_responsibilities = np.zeros((n_samples, self.n_components))\n\n        for k in range(self.n_components):\n            try:\n                # Use log space for numerical stability\n                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n            except Exception as e:\n                # Handle potential numerical issues\n                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n                log_responsibilities[:, k] = np.log(self.weights_[k] + 1e-10) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n        \n        # Convert log responsibilities to probabilities and find the max\n        return np.argmax(log_responsibilities, axis=1)\n\n    def score(self, X):\n        if self.means_ is None:\n            raise ValueError(\"Model not fitted yet.\")\n            \n        n_samples = X.shape[0]\n        log_prob = np.zeros((n_samples, self.n_components))\n        \n        for k in range(self.n_components):\n            try:\n                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True)\n            except Exception as e:\n                # Handle potential numerical issues\n                regularized_cov = self.covariances_[k] + np.eye(self.covariances_[k].shape[0]) * self.reg_covar * 10\n                log_prob[:, k] = np.log(max(self.weights_[k], 1e-10)) + multivariate_normal.logpdf(\n                    X, mean=self.means_[k], cov=regularized_cov, allow_singular=True)\n        \n        # Use logsumexp for numerical stability\n        return np.sum(logsumexp(log_prob, axis=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:03:12.745452Z","iopub.execute_input":"2025-04-30T09:03:12.745857Z","iopub.status.idle":"2025-04-30T09:03:12.776635Z","shell.execute_reply.started":"2025-04-30T09:03:12.745834Z","shell.execute_reply":"2025-04-30T09:03:12.775772Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom scipy.special import logsumexp\n\nimport numpy as np\n\ndef clusters_meaning(X, y, n_pred_clusters, n_classes):\n    print(len(np.unique(X)), len(np.unique(y)))\n    cluster_map = np.zeros((n_pred_clusters, n_classes), dtype=int)\n\n    for i in range(len(X)):\n        cluster_map[X[i], y[i]] += 1\n\n    cluster_meanings = np.zeros(n_pred_clusters, dtype=int)\n    for i, row in enumerate(cluster_map):\n        cluster_meanings[i] = np.argmax(row)\n    return cluster_meanings\n\n# Ensure X, y are already defined\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nn_components_list = [20, 40, 60]\n# n_components_list = [40]\n\nfor n in n_components_list:\n    print(f\"\\n===== n_components = {n} =====\")\n\n    # ▶️ Custom GMM\n    gmm = GMM(n_components=n, max_iter=50, tol=1e-3, n_init=3, verbose=False)\n    gmm.fit(X_train)\n    y_pred = gmm.predict(X_test)\n    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n    \n    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n    \n    print(\"\\n[GMM]\")\n    print(f\"Accuracy: {acc_sklearn:.4f}\")\n    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n\n    # ▶️ Sklearn GaussianMixture\n    gmm = GaussianMixture(n_components=n, max_iter=50, tol=1e-3, n_init=3, random_state=42)\n    gmm.fit(X_train)\n    y_pred = gmm.predict(X_test)\n    y_pred_sklearn_mapped = clusters_meaning(y_pred, y_test, n, 40)[y_pred]\n    \n    acc_sklearn = accuracy_score(y_test, y_pred_sklearn_mapped)\n    f1_sklearn = f1_score(y_test, y_pred_sklearn_mapped, average='macro')\n\n    print(\"\\n[Sklearn GMM]\")\n    print(f\"Accuracy: {acc_sklearn:.4f}\")\n    print(f\"F1 Score:  {f1_sklearn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_sklearn_mapped))\n\n    # ▶️ KNN\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    y_pred_knn = knn.predict(X_test)\n\n    acc_knn = accuracy_score(y_test, y_pred_knn)\n    f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n\n    print(\"\\n[KNN]\")\n    print(f\"Accuracy: {acc_knn:.4f}\")\n    print(f\"F1 Score:  {f1_knn:.4f}\")\n    print(confusion_matrix(y_test, y_pred_knn))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:10:09.447052Z","iopub.execute_input":"2025-04-30T09:10:09.447896Z","iopub.status.idle":"2025-04-30T09:10:15.405436Z","shell.execute_reply.started":"2025-04-30T09:10:09.447858Z","shell.execute_reply":"2025-04-30T09:10:15.404272Z"}},"outputs":[{"name":"stdout","text":"\n===== n_components = 20 =====\n10 26\n\n[GMM]\nAccuracy: 0.3750\nF1 Score:  0.2213\n[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n16 26\n\n[Sklearn GMM]\nAccuracy: 0.5750\nF1 Score:  0.4219\n[[0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n\n[KNN]\nAccuracy: 0.2750\nF1 Score:  0.2225\n[[0 0 0 ... 0 0 0]\n [0 1 0 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 1 0]\n [0 0 0 ... 0 0 0]]\n\n===== n_components = 40 =====\n24 26\n\n[GMM]\nAccuracy: 0.7000\nF1 Score:  0.6564\n[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 3 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n26 26\n\n[Sklearn GMM]\nAccuracy: 0.7750\nF1 Score:  0.7493\n[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 2 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n\n[KNN]\nAccuracy: 0.0250\nF1 Score:  0.0093\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n===== n_components = 60 =====\n28 26\n\n[GMM]\nAccuracy: 0.8500\nF1 Score:  0.8314\n[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 3 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n26 26\n\n[Sklearn GMM]\nAccuracy: 0.8500\nF1 Score:  0.8647\n[[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 3 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n\n[KNN]\nAccuracy: 0.0250\nF1 Score:  0.0147\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}